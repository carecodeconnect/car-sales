{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Sales Project\n",
    "\n",
    "## Working \n",
    "\n",
    "I quickly reviewed the task, the time required for completion, and thought about the tools I would need to complete the task.\n",
    "\n",
    "I knew the job role required MySQL and Python, and after seeing the `CSV` files contained tabular data, and looking at the task questions, I decided to use these as my main tools.\n",
    "\n",
    "As the task needed to be done in 2 hours, I decided the quickest workflow would have been using `Tableau`, but as I only have a free account with `Tableau Public`, I decided on the quickest workflow using Free and Open Source tools:\n",
    "\n",
    "- `Jupyter` notebooks to document my working and do exploratory data analysis (EDA)\n",
    "- `pandas` for data importing and wrangling the `CSV` files\n",
    "- `pandasql` to run SQL queries on the `pandas` DataFrame, which is easier syntax for basic queries\n",
    "- `pygwalker` to create quick interactive data visualisations (EDA) for the stakeholders\n",
    "- `streamlit` to make the visualisations accessible for non-tehnical stakeholders\n",
    "\n",
    "In the end, I used `Quarto` instead of `streamlit`, because I wanted to try creating a presentation/report which could be version controlled. I wanted to see if `pygwalker` would work inside a `Quarto` document.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Create project folder structure\n",
    "2. Initialise `git` repository\n",
    "3. Create `conda` environment with Python 3.10 and key packages\n",
    "4. Start inspecting data\n",
    "5. EDA cycles to visualise, inspect, and make any transformations to the data.\n",
    "6. Drafted code then used GPT-4o as code assistant when I forgot/got stuff wrong.\n",
    "7. Decide on appropriate visualisations relevant to answering the questions.\n",
    "8. Write and publish the report/presentation as Quarto documents.\n",
    "\n",
    "## Discoveries\n",
    "\n",
    "### Tables\n",
    "\n",
    "I know straight away that to answer the questions, I will need to do some SQL joins, because there is more than one table.\n",
    "\n",
    "There are two tables (DataFrames) of car sales data: `purchase_data` and `vehicle_data`\n",
    "\n",
    "`customer_id` is the primary key of the `purchase_data` table\n",
    "\n",
    "`vehicle_id` is the primary key of the `vehicle_data` table\n",
    "\n",
    "So, I will use `vehicle_id` as the key to join the tables on.\n",
    "\n",
    "### `purchase_data`\n",
    "\n",
    "This table contains:\n",
    "\n",
    "- Information about car purchases per customer\n",
    "- `9` columns\n",
    "- `2_000_000` purchases (rows)\n",
    "\n",
    "### `test_vehicle_data`\n",
    "\n",
    "- Information about vehicles\n",
    "- `19` columns\n",
    "- `978` vehicles (rows)\n",
    "\n",
    "### Answering the Questions\n",
    "\n",
    "Next, because there is a lot of data in the tables, I read the questions, to know which variables are needed to answer them.\n",
    "\n",
    "I listed the questions, highlighted the variables, and then clarified which columns refer to which variables in the tables.\n",
    "\n",
    "I need to do this to check if there are any issues with data quality, missing values, or outliers, only for the relevant variables. It will take too long to look at all of the columns, due to the number of columns.\n",
    "\n",
    "## TODO\n",
    "\n",
    "**DONE** - Tables are already indexed using the `customer_id` (`purchase_data`) and `vehicle_id` (`vehicle_data`) columns, so I need to import the `CSV` to specify the index (and not add another index).\n",
    "\n",
    "# EDA\n",
    "\n",
    "See the `01-eda.ipynb` for my working.\n",
    "\n",
    "# Overall Evaluation\n",
    "\n",
    "Strengths:\n",
    "\n",
    "- Automated workflow which means I could make reporting into a pipeline\n",
    "- I used Free and Open Source tools, so avoided paying subscriptions\n",
    "- Version control with `Quarto` is brilliant and possible compared to `Jupyter` notebooks.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "- Coding is slower than using `Tableau` GUI\n",
    "- Choosing between SQL and pandas for queries took a bit of time\n",
    "- `duckdb` would have been much faster than `pandasql`, but for some reason, it didn't work\n",
    "- Folium/Leafleat could not display map (`for` loop took too long), so went with `PyGWalker` (which sadly did not work with Quarto; I've made an issue on their GitHub), and then settled on datashader/holoviews/bokeh (efficient, but the map looked a bit crap, so I took a screenshot of the `PyGWalker` map)\n",
    "\n",
    "Ideas for next time:\n",
    "\n",
    "- `Tableau` for quicker workflow\n",
    "- `PyGWalker` for entire project as interactive visualisations \n",
    "- `Streamlit` for the app\n",
    "- `duckdb` to speed up SQL queries\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
